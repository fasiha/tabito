# Tabito

Very work in progress.

## Setup
First-time setup:
```sh
npx pnpm install       # install dependencies
npm run server:schema  # convert SQL schema to TypeScript interfaces
```

Then, start [Curtiz Japanese NLP](https://github.com/fasiha/curtiz-japanese-nlp) on `locahost`, port 8133 (or provide URL via an environment variable `CURTIZ_URL` to the following).

Then to start the dev server:
```sh
npm run dev
```
and visit http://localhost:4321.

To clear all NLP data and reload it (for example, when the NLP pipeline has changed), run
```sh
sqlite3 sentences.db "UPDATE sentence SET jsonEncoded = json_remove(jsonEncoded, '$.nlp')" && npm run dev
```
To clear all saved vocabulary:
```sh
sqlite3 sentences.db "UPDATE sentence SET jsonEncoded = json_remove(jsonEncoded, '$.vocab')" && npm run dev
```

Adapting this to just clearing the NLP data for a single sentence:
```sh
sqlite3 sentences.db "UPDATE sentence SET jsonEncoded = json_remove(jsonEncoded, '$.nlp') WHERE plain='ãªã«ã‹èª¿ã¹ã‚‚ã®ã‚’ã—ã«æ¥ãŸã®ã‹'" && npm run dev
```

> ğŸš¨ Danger âš ï¸: to delete an entire sentence:
> ```sh
>  sqlite3 sentences.db "DELETE FROM sentence WHERE plain='æŠ‘ãˆã‚‰ã‚Œãªããªã‚‹'"
> ```

## Working notes

- in the vocab memory, track what senses we've learned
- we need *directed* graphs to indicate which vocab "include" other smaller vocab, e.g., "ã‹ãœã‚’ã²ã" (to catch a cold) is a word, and reviewing it means we've also indirectly reviewed "ã‹ãœ (a cold) and "ã²ã" (to catch).
  - However, "ã²ã" has *lots* of meanings. Reviewing "ã‹ãœã‚’ã²ã" shouldn't mean you've reviewed *all* those meanings, just the one that relates to catching (a cold). So I think we need to have multiple Ebisu models, not just for meaning-to-reading and vice versa, but per individual meanings, readings, and kanji forms we've learned?